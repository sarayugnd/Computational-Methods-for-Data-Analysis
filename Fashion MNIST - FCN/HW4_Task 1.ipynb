{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c65a39",
   "metadata": {},
   "source": [
    "**HW 4**: *The goal in this assignment is to train Fully Connected Deep Neural Networks (FCNs / DNNs) to classify\n",
    "images in FashionMNIST data set. As in the MNIST dataset, each sample is a grayscale 28 × 28 image and the training set consists of 60K images and the testing set consists of 10K images. Images depict articles of clothing that belong to a class from one of\n",
    "10 classes.*\n",
    "\n",
    "**Task 1:** Design your FCN model to have an adjustable number of hidden layers and neurons in each layer with\n",
    "relu activation. The first (input) layer should be of dimension 784 and the last (output) layer of 10\n",
    "corresponding to 10 classes. Use Cross Entropy loss to perform the classification and SGD optimizer\n",
    "with learning rate as an adjustable parameter. Set the number of epochs as an adjustable parameter\n",
    "and train the network. Inspect the training loss curve and include validation of accuracy, on the\n",
    "validation set, at each epoch. Perform testing at the end of training.\n",
    "\n",
    "\n",
    "**Task 2:** Find a configuration of the FCN (by varying the adjustable parameters) that trains in reasonable time\n",
    "(several minutes is reasonable) and results in reasonable training loss curve and testing accuracy (at\n",
    "least above 85% testing accuracy). These parameters will be your baseline configuration/parameters. \n",
    "\n",
    "The model below is the basline, with defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18cd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9205fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to load and normalize the dataset for training and testing\n",
    "# It will downlad the dataset into data subfolder (change to your data folder name)\n",
    "train_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "# Use the following code to create a validation set of 10%\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_dataset)),\n",
    "    train_dataset.targets,\n",
    "    stratify=train_dataset.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Generate training and validation subsets based on indices\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "\n",
    "# set batches sizes\n",
    "train_batch_size = 900 #Define train batch size\n",
    "test_batch_size  = 1000 #Define test batch size (can be larger than train batch size)\n",
    "\n",
    "\n",
    "# Define dataloader objects that help to iterate over batches and samples for\n",
    "# training, validation and testing\n",
    "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
    "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "                                           \n",
    "num_train_batches=len(train_batches)\n",
    "num_val_batches=len(val_batches)\n",
    "num_test_batches=len(test_batches)\n",
    "\n",
    "\n",
    "#print(num_train_batches)\n",
    "#print(num_val_batches)\n",
    "#print(num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c041792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(FCN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e0277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:57<00:00, 15.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "hidden_dims = [400, 400]  # hidden layer configuration\n",
    "\n",
    "model = FCN(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims)\n",
    "\n",
    "# Define the learning rate and epochs number\n",
    "learning_rate = 0.05\n",
    "epochs = 15\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Iterate over epochs, batches with progress bar and train+validate the FCN\n",
    "# Track the loss and validation accuracy\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0  # Initialize epoch loss\n",
    "    num_batches = 0   # Initialize number of batches processed in this epoch\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, input_dim)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate loss for each batch\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss \n",
    "    \n",
    "    #print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {average_epoch_loss:.4f}')\n",
    "\n",
    "    # FCN Validation\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "\n",
    "        # Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            # Reshape validation images into a vector\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "\n",
    "            # Compute validation outputs (targets)\n",
    "            val_outputs = model(val_features)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)  # Counting total samples\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation accuracy for the epoch\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "    #print(\"Epoch:\", epoch, \"; Validation Accuracy:\", average_val_acc, '%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b196d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for computing accuracy\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 28*28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "\n",
    "# Compute total accuracy\n",
    "test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "# Report total accuracy\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6c4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style = 'whitegrid', font_scale = 1.5)\n",
    "#print(validation_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac999954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, linewidth = 3)\n",
    "#axes[0].set_ylabel(\"training loss\")\n",
    "#axes[0].set_xlabel(\"epochs\")\n",
    "\n",
    "#axes[1].plot(validation_accuracy_list, linewidth = 3, color = 'gold')\n",
    "#axes[1].set_ylabel(\"validation accuracy\")\n",
    "#sns.despine()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
