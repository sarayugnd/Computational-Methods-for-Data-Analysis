{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7423a4",
   "metadata": {},
   "source": [
    "**Task 3:** \n",
    "(a) Consider different optimizers including RMSProp, Adam and SGD with different learning rates.\n",
    "You can find the corresponding functions in the torch.optim library. Log your training loss,\n",
    "validation and test accuracy. Compare these optimizers, and observe which optimizer is most\n",
    "suitable for your FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b97a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f77aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to load and normalize the dataset for training and testing\n",
    "# It will downlad the dataset into data subfolder (change to your data folder name)\n",
    "train_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "# Use the following code to create a validation set of 10%\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_dataset)),\n",
    "    train_dataset.targets,\n",
    "    stratify=train_dataset.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Generate training and validation subsets based on indices\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "\n",
    "# set batches sizes\n",
    "train_batch_size = 900 #Define train batch size\n",
    "test_batch_size  = 1000 #Define test batch size (can be larger than train batch size)\n",
    "\n",
    "\n",
    "# Define dataloader objects that help to iterate over batches and samples for\n",
    "# training, validation and testing\n",
    "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
    "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "                                           \n",
    "num_train_batches=len(train_batches)\n",
    "num_val_batches=len(val_batches)\n",
    "num_test_batches=len(test_batches)\n",
    "\n",
    "\n",
    "#print(num_train_batches)\n",
    "#print(num_val_batches)\n",
    "#print(num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc278620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(FCN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4f3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:07<00:00, 20.47s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [06:34<00:00, 26.30s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:32<00:00, 22.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:03<00:00, 16.26s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:50<00:00, 15.35s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:16<00:00, 17.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:30<00:00, 18.01s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:31<00:00, 18.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:39<00:00, 18.61s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:36<00:00, 18.40s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:22<00:00, 17.52s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:31<00:00, 18.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': optim.SGD,\n",
    "    'RMSprop': optim.RMSprop,\n",
    "    'Adam': optim.Adam\n",
    "}\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs = 15\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over optimizer configurations\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        model = FCN(input_dim=784, output_dim=10, hidden_dims=[400, 400])\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize lists to store metrics\n",
    "        train_loss_list = np.zeros((epochs,))\n",
    "        validation_loss_list = np.zeros((epochs,))\n",
    "        validation_accuracy_list = np.zeros((epochs,))\n",
    "        test_accuracy = 0\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0  # Initialize epoch loss\n",
    "            num_batches = 0   # Initialize number of batches processed in this epoch\n",
    "            for train_features, train_labels in train_batches:\n",
    "                optimizer.zero_grad()\n",
    "                train_features = train_features.reshape(-1, input_dim)\n",
    "                outputs = model(train_features)\n",
    "                loss = loss_func(outputs, train_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()  # Accumulate loss for each batch\n",
    "                num_batches += 1\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            average_epoch_loss = epoch_loss / num_batches\n",
    "            train_loss_list[epoch] = average_epoch_loss \n",
    "\n",
    "            # Validate the model\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0\n",
    "            total_samples = 0\n",
    "            for val_features, val_labels in val_batches:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_features = val_features.reshape(-1, 28*28)\n",
    "                    val_outputs = model(val_features)\n",
    "                    loss = loss_func(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(val_outputs.data, 1)\n",
    "                    val_acc += (predicted == val_labels).sum().item()\n",
    "                    total_samples += val_labels.size(0)\n",
    "            average_val_loss = val_loss / len(val_batches)\n",
    "            average_val_acc = val_acc / total_samples * 100\n",
    "            # Record average validation loss and accuracy for the epoch\n",
    "            validation_loss_list[epoch] = average_val_loss\n",
    "            validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "        # Test the model\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for test_features, test_labels in test_batches:\n",
    "                model.eval()\n",
    "                test_features = test_features.reshape(-1, 28*28)\n",
    "                test_outputs = model(test_features)\n",
    "                _, predicted = torch.max(test_outputs, 1)\n",
    "                total_correct += (predicted == test_labels).sum().item()\n",
    "                total_samples += test_labels.size(0)\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Optimizer': optimizer_name,\n",
    "            'Learning Rate': lr,\n",
    "            'Train Loss': train_loss_list,\n",
    "            'Validation Loss': validation_loss_list,\n",
    "            'Validation Accuracy': validation_accuracy_list,\n",
    "            'Test Accuracy': test_accuracy\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f60e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "#for result in results:\n",
    "    #print(\"Optimizer:\", result['Optimizer'])\n",
    "    #print(\"Learning Rate:\", result['Learning Rate'])\n",
    "    #print (\"Validation Accuracy:\", result['Validation Accuracy'])\n",
    "    #print(\"Test Accuracy:\", result['Test Accuracy'])\n",
    "    #print(\"Training loss:\", result['Train Loss'])\n",
    "    #print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44743988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.set(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Filter results for Adam optimizer with learning rate 0.001\n",
    "#adam_results = [result for result in results if result['Optimizer'] == 'Adam' and result['Learning Rate'] == 0.001]\n",
    "\n",
    "# Extract data\n",
    "#adam_train_loss = adam_results[0]['Train Loss']\n",
    "#adam_validation_accuracy = adam_results[0]['Validation Accuracy']\n",
    "#adam_validation_loss = adam_results[0]['Validation Loss']\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(adam_train_loss, label='Training Loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss for Adam (lr=0.001)')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('Loss')\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(adam_validation_accuracy, label='Validation Accuracy', linewidth=3, color='gold')\n",
    "#axes[1].set_title('Validation Accuracy for Adam (lr=0.001)')\n",
    "#axes[1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375c1e3",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "(b) Analyze the overfitting/underfitting situation of your model. Include Dropout regularization and\n",
    "discuss whether this improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "655cedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, dropout_rate):\n",
    "        super(FCNWithDropout, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "            x = self.dropout(x) \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24a74d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:07<00:00, 16.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with dropout\n",
    "dropout_rate = 0.5\n",
    "model_with_dropout = FCNWithDropout(input_dim=784, output_dim=10, hidden_dims=[400, 400], dropout_rate=dropout_rate)\n",
    "optimizer = optim.Adam(model_with_dropout.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_loss_list_with_dropout = []\n",
    "validation_accuracy_list_with_dropout = []\n",
    "validation_loss_list_with_dropout = [0] * epochs\n",
    "test_accuracy = 0\n",
    "\n",
    "# Train the model with dropout\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_with_dropout.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, 784)\n",
    "        outputs = model_with_dropout(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()  # Accumulate loss for each batch\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list_with_dropout.append(average_epoch_loss)\n",
    "    #print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {average_epoch_loss:.4f}')\n",
    "    \n",
    "    # Validate the model\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model_with_dropout.eval()\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "            val_outputs = model_with_dropout(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list_with_dropout[epoch] = average_val_loss\n",
    "    validation_accuracy_list_with_dropout.append(average_val_acc)\n",
    "    #print(\"Epoch:\", epoch, \"; Validation Accuracy:\", validation_accuracy_list_with_dropout[epoch], '%')\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "        for test_features, test_labels in test_batches:\n",
    "            model_with_dropout.eval()\n",
    "            test_features = test_features.reshape(-1, 28*28)\n",
    "            test_outputs = model_with_dropout.eval()(test_features)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            total_correct += (predicted == test_labels).sum().item()\n",
    "            total_samples += test_labels.size(0)\n",
    "test_accuracy_with = total_correct / total_samples * 100\n",
    "#print(\"Test Accuracy:\", test_accuracy_with, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d4adb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(adam_train_loss, label='Without Dropout', linewidth=3)\n",
    "#axes[0].plot(train_loss_list_with_dropout, label='With Dropout', linewidth=3)\n",
    "#axes[0].set_title('Training Loss for Adam (lr=0.001) with vs without dropout')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('Training Loss')\n",
    "#axes[0].legend()\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(adam_validation_accuracy, label='Without Dropout', linewidth=3, color='gold')\n",
    "#axes[1].plot(validation_accuracy_list_with_dropout, label='With Dropout', linewidth=3)\n",
    "#axes[1].set_title('Validation Accuracy for Adam (lr=0.001) with vs without dropout')\n",
    "#axes[1].set_ylabel('Validation Accuracy')\n",
    "#axes[1].legend()\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd3725d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(adam_train_loss, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(adam_validation_loss, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss without dropout')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()\n",
    "\n",
    "#axes[1].plot(train_loss_list_with_dropout, label='Training loss with dropout', linewidth=3)\n",
    "#axes[1].plot(validation_loss_list_with_dropout, label='Validation loss with dropout', linewidth=3)\n",
    "#axes[1].set_title('Training Loss vs Validation Loss with dropout')\n",
    "#axes[1].set_xlabel('Epochs')\n",
    "#axes[1].set_ylabel('loss')\n",
    "#axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56befaa",
   "metadata": {},
   "source": [
    "**Task 3:** (c) Consider different Initializations, such Random Normal, Xavier Normal, Kaiming (He) Uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14546477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, initialization='xavier_normal'):\n",
    "        super(FCN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Initialize weights based on the chosen initialization method\n",
    "        if initialization == 'random_normal':\n",
    "            self.initialize_weights_random_normal()\n",
    "        elif initialization == 'xavier_normal':\n",
    "            self.initialize_weights_xavier_normal()\n",
    "        elif initialization == 'kaiming_uniform':\n",
    "            self.initialize_weights_kaiming_uniform()\n",
    "\n",
    "    def initialize_weights_random_normal(self):\n",
    "        for layer in self.hidden_layers:\n",
    "            init.normal_(layer.weight.data, mean=0.0, std=1.0)\n",
    "            init.constant_(layer.bias.data, 0.0)\n",
    "        init.normal_(self.output_layer.weight.data, mean=0.0, std=1.0)\n",
    "        init.constant_(self.output_layer.bias.data, 0.0)\n",
    "\n",
    "    def initialize_weights_xavier_normal(self):\n",
    "        for layer in self.hidden_layers:\n",
    "            init.xavier_normal_(layer.weight.data)\n",
    "            init.constant_(layer.bias.data, 0.0)\n",
    "        init.xavier_normal_(self.output_layer.weight.data)\n",
    "        init.constant_(self.output_layer.bias.data, 0.0)\n",
    "\n",
    "    def initialize_weights_kaiming_uniform(self):\n",
    "        for layer in self.hidden_layers:\n",
    "            init.kaiming_uniform_(layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            init.constant_(layer.bias.data, 0.0)\n",
    "        init.kaiming_uniform_(self.output_layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "        init.constant_(self.output_layer.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfff651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: random_normal: 100%|█████████████████████████████████████████████████████| 15/15 [07:58<00:00, 31.90s/it]\n",
      "Optimization: xavier_normal: 100%|█████████████████████████████████████████████████████| 15/15 [08:00<00:00, 32.03s/it]\n",
      "Optimization: kaiming_uniform: 100%|███████████████████████████████████████████████████| 15/15 [07:59<00:00, 31.97s/it]\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "hidden_dims = [400, 400]\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over optimizer configurations\n",
    "initializations = ['random_normal', 'xavier_normal', 'kaiming_uniform']\n",
    "\n",
    "for initialization in initializations:\n",
    "    # Initialize model\n",
    "    model = FCN(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims, initialization=initialization)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loss_list = []\n",
    "    validation_loss_list = []\n",
    "    validation_accuracy_list = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(epochs), desc=f'Optimization: {initialization}'):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0  # Initialize epoch loss\n",
    "        num_batches = 0   # Initialize number of batches processed in this epoch\n",
    "        for train_features, train_labels in train_batches:\n",
    "            optimizer.zero_grad()\n",
    "            train_features = train_features.reshape(-1, input_dim)\n",
    "            outputs = model(train_features)\n",
    "            loss = loss_func(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate loss for each batch\n",
    "            num_batches += 1\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        average_epoch_loss = epoch_loss / num_batches\n",
    "        train_loss_list.append(average_epoch_loss)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0\n",
    "        total_samples = 0\n",
    "        for val_features, val_labels in val_batches:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_features = val_features.reshape(-1, 28*28)\n",
    "                val_outputs = model(val_features)\n",
    "                loss = loss_func(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(val_outputs.data, 1)\n",
    "                val_acc += (predicted == val_labels).sum().item()\n",
    "                total_samples += val_labels.size(0)\n",
    "        average_val_loss = val_loss / len(val_batches)\n",
    "        average_val_acc = val_acc / total_samples * 100\n",
    "        # Record average validation loss and accuracy for the epoch\n",
    "        validation_loss_list.append(average_val_loss)\n",
    "        validation_accuracy_list.append(average_val_acc)\n",
    "\n",
    "    # Test the model\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for test_features, test_labels in test_batches:\n",
    "            model.eval()\n",
    "            test_features = test_features.reshape(-1, 28*28)\n",
    "            test_outputs = model(test_features)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            total_correct += (predicted == test_labels).sum().item()\n",
    "            total_samples += test_labels.size(0)\n",
    "    test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Initialization': initialization,\n",
    "        'Train Loss': train_loss_list,\n",
    "        'Validation Loss': validation_loss_list,\n",
    "        'Validation Accuracy': validation_accuracy_list,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "227b5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "#for result in results:\n",
    "    #print(\"Initialization:\", result['Initialization'])\n",
    "    #print(\"Validation Loss:\", result['Validation Loss'])\n",
    "    #print (\"Validation Accuracy:\", result['Validation Accuracy'])\n",
    "    #print(\"Test Accuracy:\", result['Test Accuracy'])\n",
    "    #print(\"Training loss:\", result['Train Loss'])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d7f5e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n",
    "\n",
    "#for i, result in enumerate(results):\n",
    "    #initialization = result['Initialization']\n",
    "    #axes[i, 0].plot(result['Train Loss'], label='Training loss', linewidth=3)\n",
    "    #axes[i, 0].plot(result['Validation Loss'], label='Validation loss', linewidth=3)\n",
    "    #axes[i, 0].set_title(f'Training Loss vs Validation Loss - {initialization}')\n",
    "    #axes[i, 0].set_xlabel('Epochs')\n",
    "    #axes[i, 0].set_ylabel('Loss')\n",
    "    #axes[i, 0].legend()\n",
    "\n",
    "    #axes[i, 1].plot(result['Validation Accuracy'], label='Validation Accuracy', linewidth=3)\n",
    "    #axes[i, 1].set_title(f'Validation Accuracy - {initialization}')\n",
    "    #axes[i, 1].set_xlabel('Epochs')\n",
    "    #axes[i, 1].set_ylabel('Accuracy (%)')\n",
    "    #axes[i, 1].legend()\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4bcda",
   "metadata": {},
   "source": [
    "**Task 3:** (d) Include normalization such as Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a61b21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(FCNWithBatchNorm, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dims[0])])  # BatchNorm layer for the first hidden layer\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dims[i + 1]))  # BatchNorm layer for subsequent hidden layers\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, batch_norm in zip(self.hidden_layers, self.batch_norms):\n",
    "            x = layer(x)\n",
    "            x = batch_norm(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54447857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:41<00:00, 14.79s/it]\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "hidden_dims = [400, 400]\n",
    "\n",
    "model = FCNWithBatchNorm(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "results = {}\n",
    "    \n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "validation_accuracy_list = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0  # Initialize epoch loss\n",
    "    num_batches = 0   # Initialize number of batches processed in this epoch\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, input_dim)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate loss for each batch\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list.append(average_epoch_loss)\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list.append(average_val_loss)\n",
    "    validation_accuracy_list.append(average_val_acc)\n",
    "\n",
    "# Test the model\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        test_features = test_features.reshape(-1, 28*28)\n",
    "        test_outputs = model(test_features)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        total_samples += test_labels.size(0)\n",
    "test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "# Store results\n",
    "results['FCNWithBatchNorm'] = {\n",
    "    'Train Loss': train_loss_list,\n",
    "    'Validation Loss': validation_loss_list,\n",
    "    'Validation Accuracy': validation_accuracy_list,\n",
    "    'Test Accuracy': test_accuracy\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef21a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model_name, result in results.items():\n",
    "    #print(\"Model:\", model_name)\n",
    "    #print(\"Validation Loss:\", result['Validation Loss'])\n",
    "    #print (\"Validation Accuracy:\", result['Validation Accuracy'])\n",
    "    #print(\"Test Accuracy:\", result['Test Accuracy'])\n",
    "    #print(\"Training loss:\", result['Train Loss'])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "031f763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, label='Training Loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss for Adam (lr=0.001) with BatchNorm')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('Loss')\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, label='Validation Accuracy', linewidth=3, color='gold')\n",
    "#axes[1].set_title('Validation Accuracy for Adam (lr=0.001) with BatchNorm')\n",
    "#axes[1].set_ylabel('Validation Accuracy')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87d03bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation Loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ddb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
