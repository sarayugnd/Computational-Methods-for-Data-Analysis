{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385e0427",
   "metadata": {},
   "source": [
    "**Task 3:** *Implement and train a CNN 100K model with convolutional, pooling, and FC layers with up to 100K\n",
    "weights. Perform hyperparameter tuning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e54afce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d055aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "# Use the following code to create a validation set of 10%\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_dataset)),\n",
    "    train_dataset.targets,\n",
    "    stratify=train_dataset.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Generate training and validation subsets based on indices\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "\n",
    "# set batches sizes\n",
    "train_batch_size = 900 #Define train batch size\n",
    "test_batch_size  = 1000 #Define test batch size (can be larger than train batch size)\n",
    "\n",
    "\n",
    "# Define dataloader objects that help to iterate over batches and samples for\n",
    "# training, validation and testing\n",
    "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
    "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "                                           \n",
    "num_train_batches=len(train_batches)\n",
    "num_val_batches=len(val_batches)\n",
    "num_test_batches=len(test_batches)\n",
    "\n",
    "\n",
    "#print(num_train_batches)\n",
    "#print(num_val_batches)\n",
    "#print(num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bf68077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 120)  \n",
    "        self.fc2 = nn.Linear(120, output_dim)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 7 * 7)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8570c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 96658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:35<00:00, 22.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_channels = 1\n",
    "output_dim = 10\n",
    "#hidden_dims = [64, 64]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 100000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "#model = CNN(num_channels_conv1=32, num_channels_conv2=64, num_neurons_fc1=128, num_neurons_fc2=10)\n",
    "model = CNN(input_channels, output_dim)\n",
    "\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, 1, 28, 28)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 1, 28, 28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17785730",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_list = []\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "        # Compute total accuracy\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "        #print(\"Test Accuracy:\", test_accuracy, \"%\")\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "#test_accuracy_array = np.array(test_accuracy_list)\n",
    "#test_accuracy_std = np.std(test_accuracy_array)\n",
    "\n",
    "# Calculate upper and lower bounds of testing accuracy\n",
    "#test_accuracy_upper_bound = np.max(test_accuracy_array)\n",
    "#test_accuracy_lower_bound = np.min(test_accuracy_array)\n",
    "\n",
    "#print(\"Standard Deviation of Testing Accuracy:\", test_accuracy_std)\n",
    "#print(\"Upper Bound of Testing Accuracy:\", test_accuracy_upper_bound)\n",
    "#print(\"Lower Bound of Testing Accuracy:\", test_accuracy_lower_bound)\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e96cdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create subplots\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12580e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()\n",
    "\n",
    "#axes[1].plot(test_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_title('Testing Loss')\n",
    "#axes[1].set_ylabel(\"Loss\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5498a1f",
   "metadata": {},
   "source": [
    "**Task 4:** *Reduce the number of weights in the CNN 100K model to create CNN 50K, CNN 20K, and CNN 10K.\n",
    "Train these models similarly to CNN 100K.*\n",
    "\n",
    "For CNN 50K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c95552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 60)  \n",
    "        self.fc2 = nn.Linear(60, output_dim)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 7 * 7)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b434fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 48958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:27<00:00, 21.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_channels = 1\n",
    "output_dim = 10\n",
    "#hidden_dims = [64, 64]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 100000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "#model = CNN(num_channels_conv1=32, num_channels_conv2=64, num_neurons_fc1=128, num_neurons_fc2=10)\n",
    "model = CNN(input_channels, output_dim)\n",
    "\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, 1, 28, 28)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 1, 28, 28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "        \n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "efficiency = total_params / training_time\n",
    "#print(\"Weights per second:\", efficiency)\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25dfe6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_list = []\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "        # Compute total accuracy\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "        #print(\"Test Accuracy:\", test_accuracy, \"%\")\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cf39710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create subplots\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abdb33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c456ed",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "\n",
    "For CNN 20K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4095e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 45)  \n",
    "        self.fc2 = nn.Linear(45, output_dim)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e43aa68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 18481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:33<00:00, 18.20s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_channels = 1\n",
    "output_dim = 10\n",
    "#hidden_dims = [64, 64]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 100000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "#model = CNN(num_channels_conv1=32, num_channels_conv2=64, num_neurons_fc1=128, num_neurons_fc2=10)\n",
    "model = CNN(input_channels, output_dim)\n",
    "\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, 1, 28, 28)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 1, 28, 28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "        \n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "efficiency = total_params / training_time\n",
    "#print(\"Weights per second:\", efficiency)\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96602763",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_list = []\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "        # Compute total accuracy\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "        #print(\"Test Accuracy:\", test_accuracy, \"%\")\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93dcfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create subplots\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a55cc1",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "\n",
    "For CNN 10K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d54db7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 22)  \n",
    "        self.fc2 = nn.Linear(22, output_dim)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a12eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [04:39<00:00, 18.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_channels = 1\n",
    "output_dim = 10\n",
    "#hidden_dims = [64, 64]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 100000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "model = CNN(input_channels, output_dim)\n",
    "\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, 1, 28, 28)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 1, 28, 28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "        \n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0627b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_list = []\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 1, 28, 28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "        # Compute total accuracy\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "        #print(\"Test Accuracy:\", test_accuracy, \"%\")\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc89a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "#sns.despine()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e13c9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82722e",
   "metadata": {},
   "source": [
    "**Bonus:** *Pick a CNN variant from above and then pick several input samples from different\n",
    "classes. Visualize some of the feature maps of the convolutional layers for these samples (e.g. display\n",
    "the feature maps in a grid of nxn).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a84b4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Select input samples from different classes\n",
    "sample_indices = [0, 100, 200, 300]  # Choose samples from different classes\n",
    "num_samples = len(sample_indices)\n",
    "\n",
    "# Forward pass and extract feature maps\n",
    "with torch.no_grad():\n",
    "    for idx in sample_indices:\n",
    "        input_image = test_features[idx].unsqueeze(0)  # Select input image\n",
    "        output = model(input_image)  # Forward pass\n",
    "        feature_maps = model.conv1(input_image)  # Extract feature maps from the first convolutional layer\n",
    "\n",
    "        # Visualization\n",
    "        #num_feature_maps = feature_maps.size(1)\n",
    "        #fig, axes = plt.subplots(1, num_feature_maps, figsize=(12, 2))\n",
    "\n",
    "        #for i in range(num_feature_maps):\n",
    "            #axes[i].imshow(feature_maps[0, i].detach().cpu(), cmap='gray')\n",
    "            #axes[i].axis('off')\n",
    "            #axes[i].set_title(f'Feature Map {i+1}')\n",
    "\n",
    "        #plt.suptitle(f'Input Sample {idx} (Class: {test_labels[idx]})')\n",
    "        #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
