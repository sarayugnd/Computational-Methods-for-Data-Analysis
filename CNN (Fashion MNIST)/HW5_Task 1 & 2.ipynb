{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac8e392",
   "metadata": {},
   "source": [
    "**HW5: Image Classification**\n",
    "\n",
    "**GOAL:** The goal in this assignment is similar to the goal of the previous assignment (Homework 4) in which we\n",
    "trained FCNs to classify images in the FashionMNIST data set. In this assignment, however, we have a\n",
    "budget of weights that you can incorporate into your neural network model and we will compare FCNs vs.\n",
    "Convolutional Neural Networks (CNNs).\n",
    "\n",
    "**Task 1:** *With the constraint of incorporating up to 100K weights in the design of your FCN model, perform\n",
    "hyperparameter tuning to achieve FCN model whose testing classification accuracy is above 88% on\n",
    "the testing set*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a55598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "970f8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to load and normalize the dataset for training and testing\n",
    "# It will downlad the dataset into data subfolder (change to your data folder name)\n",
    "train_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST('C:\\\\Users\\\\Sarayu G\\\\582\\\\', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "# Use the following code to create a validation set of 10%\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_dataset)),\n",
    "    train_dataset.targets,\n",
    "    stratify=train_dataset.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Generate training and validation subsets based on indices\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "\n",
    "# set batches sizes\n",
    "train_batch_size = 900 #Define train batch size\n",
    "test_batch_size  = 1000 #Define test batch size (can be larger than train batch size)\n",
    "\n",
    "\n",
    "# Define dataloader objects that help to iterate over batches and samples for\n",
    "# training, validation and testing\n",
    "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
    "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "                                           \n",
    "num_train_batches=len(train_batches)\n",
    "num_val_batches=len(val_batches)\n",
    "num_test_batches=len(test_batches)\n",
    "\n",
    "\n",
    "#print(num_train_batches)\n",
    "#print(num_val_batches)\n",
    "#print(num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df790dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FCN model class\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(FCN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dims[0])])\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfc34254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 85614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:50<00:00, 15.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "hidden_dims = [100, 64]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 100000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "model = FCN(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims)\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, input_dim)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c53cabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for computing accuracy\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 28*28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "        # Compute total accuracy\n",
    "        test_accuracy = total_correct / total_samples * 100\n",
    "        #print(\"Test Accuracy:\", test_accuracy, \"%\")\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "test_accuracy_array = np.array(test_accuracy_list)\n",
    "test_accuracy_std = np.std(test_accuracy_array)\n",
    "\n",
    "# Calculate upper and lower bounds of testing accuracy\n",
    "test_accuracy_upper_bound = np.max(test_accuracy_array)\n",
    "test_accuracy_lower_bound = np.min(test_accuracy_array)\n",
    "\n",
    "#print(\"Standard Deviation of Testing Accuracy:\", test_accuracy_std)\n",
    "#print(\"Upper Bound of Testing Accuracy:\", test_accuracy_upper_bound)\n",
    "#print(\"Lower Bound of Testing Accuracy:\", test_accuracy_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b69dcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47935029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c58ed1",
   "metadata": {},
   "source": [
    "**Task 2:** *Reduce by half the number of weights in the FCN 100K model to create FCN 50K, and also double\n",
    "the number of weights to create FCN 200K. Train these models similarly to FCN 100K and study the\n",
    "accuracy of these models by comparing the different FCN variants.*\n",
    "\n",
    "For 50K weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed5fe85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 45910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:30<00:00, 14.02s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "hidden_dims = [54, 54]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 50000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "model = FCN(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims)\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, input_dim)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "869ded4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for computing accuracy\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 28*28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "\n",
    "# Compute total accuracy\n",
    "test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "# Report total accuracy\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09bc5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create subplots\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bebecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4fa83",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "\n",
    "For 200K weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897a5fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 195834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:39<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize data dimensions and hyperparameters\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "hidden_dims = [200, 184]  # hidden layer configuration\n",
    "learning_rate = 0.003\n",
    "epochs = 15\n",
    "max_weights = 200000\n",
    "\n",
    "# Load FashionMNIST data and define train_batches, val_batches\n",
    "\n",
    "# Define the FCN model\n",
    "model = FCN(input_dim=input_dim, output_dim=output_dim, hidden_dims=hidden_dims)\n",
    "\n",
    "# Count total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in the model:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_list = np.zeros((epochs,))\n",
    "validation_loss_list = np.zeros((epochs,))\n",
    "validation_accuracy_list = np.zeros((epochs,))\n",
    "test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over epochs and train the FCN model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for train_features, train_labels in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "        train_features = train_features.reshape(-1, input_dim)\n",
    "        outputs = model(train_features)\n",
    "        loss = loss_func(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    average_epoch_loss = epoch_loss / num_batches\n",
    "    train_loss_list[epoch] = average_epoch_loss\n",
    "\n",
    "    # Evaluate validation accuracy\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total_samples = 0\n",
    "    for val_features, val_labels in val_batches:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_features = val_features.reshape(-1, 28*28)\n",
    "            val_outputs = model(val_features)\n",
    "            loss = loss_func(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "    average_val_loss = val_loss / len(val_batches)\n",
    "    average_val_acc = val_acc / total_samples * 100\n",
    "    # Record average validation loss and accuracy for the epoch\n",
    "    validation_loss_list[epoch] = average_val_loss\n",
    "    validation_accuracy_list[epoch] = average_val_acc\n",
    "\n",
    "    #print(f\"Epoch {epoch + 1}/{epochs}, Validation Accuracy: {average_val_acc}%\")\n",
    "\n",
    "    # Check if the total parameters are within the budget\n",
    "    if total_params > max_weights:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print total training time\n",
    "#print(\"Total training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba5d962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for computing accuracy\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "with torch.no_grad():\n",
    "    for test_features, test_labels in test_batches:\n",
    "        model.eval()\n",
    "        \n",
    "        # Reshape test images into a vector\n",
    "        test_features = test_features.reshape(-1, 28*28)\n",
    "        \n",
    "        # Compute test outputs (targets)\n",
    "        test_outputs = model(test_features)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        # Compute number of correct predictions in the batch\n",
    "        total_correct += (predicted == test_labels).sum().item()\n",
    "        \n",
    "        # Count total number of samples in the batch\n",
    "        total_samples += test_labels.size(0)\n",
    "\n",
    "# Compute total accuracy\n",
    "test_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "# Report total accuracy\n",
    "#print(\"Test Accuracy:\", test_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7336ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Seaborn style and font scale\n",
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create subplots\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot training loss\n",
    "#axes[0].plot(train_loss_list, linewidth=3)\n",
    "#axes[0].set_ylabel(\"Training Loss\")\n",
    "#axes[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Plot validation accuracy\n",
    "#axes[1].plot(validation_accuracy_list, linewidth=3, color='gold')\n",
    "#axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "#axes[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "# Remove the top and right spines from the plots\n",
    "#sns.despine()\n",
    "\n",
    "# Display the plots\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45aa6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "#axes[0].plot(train_loss_list, label='Training loss', linewidth=3)\n",
    "#axes[0].plot(validation_loss_list, label='Validation loss', linewidth=3)\n",
    "#axes[0].set_title('Training Loss vs Validation loss')\n",
    "#axes[0].set_xlabel('Epochs')\n",
    "#axes[0].set_ylabel('loss')\n",
    "#axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180da81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
